# Building a Brain Tumor Classifier with Deep Learning and Knowledge Representation

In this project, I aimed to combine deep learning with knowledge representation to tackle a challenging problem: brain tumor classification using MRI scans. This effort represents a culmination of the concepts and techniques I’ve learned and applied, ranging from CNN architectures to the integration of a knowledge graph for enhanced interpretability. The project taught me a lot about the challenges and nuances of applying AI in medical imaging and how interpretability plays a crucial role in real-world applications.

## **The Problem**

Brain tumor diagnosis is a critical task in medical imaging, often requiring expertise and time from radiologists. However, traditional manual diagnosis can be subjective and inconsistent. By building a convolutional neural network (CNN) based classifier, my goal was to automate the classification of brain tumors into four categories: glioma, meningioma, pituitary tumor, and no tumor. But I didn’t stop at classification—this project also focused on connecting the classified tumor types with associated symptoms using a knowledge graph.

## **What I Did**

The core of this project was building and training a CNN using the Xception architecture, which has a proven track record for image classification tasks. Using a brain tumor dataset, I split the data into training, validation, and test sets. To ensure robustness, I applied data augmentation techniques like rotations, brightness shifts, and zooming to increase variability and reduce overfitting.

Here’s a breakdown of the major steps:
1. **Data Preparation**: Preprocessed and augmented the MRI images to improve model generalization.
2. **Model Architecture**: Leveraged transfer learning with Xception, fine-tuning the model for the tumor classification task.
3. **Training**: Trained the model using strategies like early stopping and learning rate reduction to prevent overfitting.
4. **Evaluation**: Assessed the model using metrics like accuracy, precision, recall, and a confusion matrix.
5. **Knowledge Graph Integration**: Incorporated a static knowledge graph to map classified tumors to their associated symptoms, with relationships derived from trusted medical sources like the Mayo Clinic.

## **Key Results**

The model achieved:
- **Accuracy**: 97.94%
- **Precision**: 98.09%
- **Recall**: 97.94%

These results show that the model effectively distinguishes between tumor types. The confusion matrix analysis revealed high performance across all four classes, although class imbalances slightly affected scores.

Despite the high classification accuracy, I encountered challenges with the knowledge graph. I initially attempted to integrate the Google Knowledge Graph API to dynamically retrieve symptoms but found it lacked the necessary medical-specific information. As a fallback, I implemented a static knowledge graph based on pre-defined relationships supported by medical literature.

## **What I Learned**

1. **Balancing Model Performance and Interpretability**:
   The static knowledge graph made the classification outputs more interpretable by linking tumor types to symptoms. This addition highlighted the importance of combining AI predictions with domain-specific insights to improve usability.

2. **The Importance of Data Preprocessing**:
   Data augmentation played a vital role in preventing overfitting and improving the model’s generalizability.

3. **Challenges of Real-World APIs**:
   The limitations of the Google Knowledge Graph API underscored the need for domain-specific tools. This inspired me to explore future possibilities like using dedicated medical APIs for richer and more dynamic knowledge graphs.

4. **Class Imbalance Impacts**:
   The imbalance in the dataset revealed how important it is to address fairness in training. Techniques like oversampling or weighted loss functions could further improve performance.

5. **Deep Learning Isn’t a “Black Box”**:
   Through experimentation with feature mapping (which I ultimately couldn’t implement due to technical issues), I recognized the need for explainable AI in medical applications.

## **Reflections**

This project was a comprehensive exercise in applying what I’ve learned. From designing a robust neural network architecture to tackling real-world challenges with knowledge graphs, it represents both the opportunities and complexities of AI in healthcare. While the results were promising, they also highlighted areas for growth, such as incorporating more dynamic and accurate knowledge representation tools.

This experience has shown me how AI isn’t just about achieving high accuracy but also about creating systems that are interpretable, actionable, and practical for real-world use. Moving forward, I aim to explore how these insights can be generalized to other domains, bridging the gap between AI theory and impactful applications.
